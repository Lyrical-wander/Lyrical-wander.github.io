<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="本文将花近万字讲解Q-learning算法即笔者在学习Q-learning过程中遇到的难理解的地方，包括强化学习原理以及Q表更新函数等都做了详细解释，详细解释Q-learning算法的各种细节，使用gym库的FrozenLake模拟环境来训练Q-learning智能体，最后给出完整代码，构建简单，初学者也能跑！">
<title>Q-learning强化算法详解</title>

<link rel='canonical' href='https://Lyrical-wander.github.io/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/'>

<link rel="stylesheet" href="/scss/style.min.10c2e3027d0e756379cc4e5e449541d162ac1c243be22aabefdaa0fc257af76e.css"><meta property='og:title' content="Q-learning强化算法详解">
<meta property='og:description' content="本文将花近万字讲解Q-learning算法即笔者在学习Q-learning过程中遇到的难理解的地方，包括强化学习原理以及Q表更新函数等都做了详细解释，详细解释Q-learning算法的各种细节，使用gym库的FrozenLake模拟环境来训练Q-learning智能体，最后给出完整代码，构建简单，初学者也能跑！">
<meta property='og:url' content='https://Lyrical-wander.github.io/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/'>
<meta property='og:site_name' content='苏三有春的博客'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='数学建模' /><meta property='article:tag' content='数学' /><meta property='article:published_time' content='2025-01-20T13:20:00&#43;08:00'/><meta property='article:modified_time' content='2025-01-20T13:20:00&#43;08:00'/><meta property='og:image' content='https://Lyrical-wander.github.io/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image.jpg' />
<meta name="twitter:title" content="Q-learning强化算法详解">
<meta name="twitter:description" content="本文将花近万字讲解Q-learning算法即笔者在学习Q-learning过程中遇到的难理解的地方，包括强化学习原理以及Q表更新函数等都做了详细解释，详细解释Q-learning算法的各种细节，使用gym库的FrozenLake模拟环境来训练Q-learning智能体，最后给出完整代码，构建简单，初学者也能跑！"><meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content='https://Lyrical-wander.github.io/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image.jpg' />
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">

        
        <script src="https://www.lyrical-wander.cn/background/particles.js"></script>
        <link rel="stylesheet" href="https://www.lyrical-wander.cn/background/style.css">
        <div id="particles-js"></div>
        <script>
            particlesJS.load('particles-js', "https://www.lyrical-wander.cn/background/particles_config.json", function() {
                console.log('particles.js loaded - callback');
            });
        </script>
        
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu_ed3f2992e333f57f.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">🧐</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">苏三有春的博客</a></h1>
            <h2 class="site-description">人生茫茫，唯死亡与爱而已</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://space.bilibili.com/28788259'
                        target="_blank"
                        title="Bilibili"
                        rel="me"
                    >
                        
                        
                            <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-brand-bilibili"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 10a4 4 0 0 1 4 -4h10a4 4 0 0 1 4 4v6a4 4 0 0 1 -4 4h-10a4 4 0 0 1 -4 -4v-6z" /><path d="M8 3l2 3" /><path d="M16 3l-2 3" /><path d="M9 13v-2" /><path d="M15 11v2" /></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://blog.csdn.net/whale_cat'
                        target="_blank"
                        title="CSDN"
                        rel="me"
                    >
                        
                        
                            <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-brand-coinbase"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12.95 22c-4.503 0 -8.445 -3.04 -9.61 -7.413c-1.165 -4.373 .737 -8.988 4.638 -11.25a9.906 9.906 0 0 1 12.008 1.598l-3.335 3.367a5.185 5.185 0 0 0 -7.354 .013a5.252 5.252 0 0 0 0 7.393a5.185 5.185 0 0 0 7.354 .013l3.349 3.367a9.887 9.887 0 0 1 -7.05 2.912z" /></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://github.com/Lyrical-wander'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%85%B3%E4%BA%8E%E6%88%91/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于我</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>归档</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>搜索</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>友情链接</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">
                    
                        <li id="i18n-switch">  
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                            <select name="language" title="language" onchange="window.location.href = this.selectedOptions[0].value">
                                
                                    <option value="https://Lyrical-wander.github.io/en/" >English</option>
                                
                                    <option value="https://Lyrical-wander.github.io/" selected>中文</option>
                                
                            </select>
                        </li>
                    
                

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#前言">前言</a></li>
    <li><a href="#强化学习">强化学习</a>
      <ol>
        <li><a href="#什么是强化学习">什么是强化学习</a></li>
        <li><a href="#智能体">智能体</a></li>
        <li><a href="#环境">环境</a></li>
        <li><a href="#强化学习原理">强化学习原理</a></li>
      </ol>
    </li>
    <li><a href="#q-learning">Q-learning</a></li>
    <li><a href="#q-learning智能体">Q-learning智能体</a>
      <ol>
        <li><a href="#参数解释">参数解释</a></li>
        <li><a href="#行为选择">行为选择</a></li>
        <li><a href="#q表更新">Q表更新</a></li>
      </ol>
    </li>
    <li><a href="#forzenlake模拟环境">ForzenLake模拟环境</a></li>
    <li><a href="#总结">总结</a></li>
    <li><a href="#完整代码">完整代码</a></li>
    <li><a href="#参考文章">参考文章</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="has-image main-article">
    <header class="article-header">
        <div class="article-image">
            <a href="/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/">
                <img src="/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image_hu_3f1b7cb38f6861ae.jpg"
                        srcset="/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image_hu_3f1b7cb38f6861ae.jpg 800w, /p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image_hu_93ab07427016b3e7.jpg 1600w"
                        width="800" 
                        height="650" 
                        loading="lazy"
                        alt="Featured image of post Q-learning强化算法详解" />
                
            </a>
        </div>
    

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/%E7%AE%97%E6%B3%95/" style="background-color: #2a9d8f; color: #fff;">
                算法
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/">Q-learning强化算法详解</a>
        </h2>
    
        
        <h3 class="article-subtitle">
            本文将花近万字讲解Q-learning算法即笔者在学习Q-learning过程中遇到的难理解的地方，包括强化学习原理以及Q表更新函数等都做了详细解释，详细解释Q-learning算法的各种细节，使用gym库的FrozenLake模拟环境来训练Q-learning智能体，最后给出完整代码，构建简单，初学者也能跑！
        </h3>
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Jan 20, 2025</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    阅读时长: 23 分钟
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="前言">前言
</h2><p>Hi,everyone，相信现在看到这篇文章的程序员或者未来的程序员们，或多或少在曾经某个懵懂的青春岁月里都想过“我以后要做一款属于自己的游戏！”这样美好的念头。当然，时过境迁，有的忘记了曾经的誓言，有的当一个笑话，不管怎么说，今天我们来聊聊，但不是说我们要开始制作出LOL那样的MOBA类游戏，也不是CS那样的FPS游戏，甚至不是一款游戏，而是游戏的一部分——游戏AI。</p>
<p>当然，游戏AI也有多种多样的，有与人进行博弈的，有伪装成玩家的解决游戏问题的，甚至，在大语言模型盛行的今天，也有进行文字情感交流的。但归根结底，笔者认为它们都有一些共通点：</p>
<ul>
<li>能够获取环境输入</li>
<li>能够识别并处理环境</li>
<li>输出一个行为</li>
<li>行为改变环境并获得一些反馈</li>
</ul>
<p>如三国志（顺便一提，笔者非常喜欢玩三国志13和14），总而言之呢，我们的目标，就是设计一个智能体，它能获得环境（游戏内的场景战况等whatever）的输入，能处理环境，并输出一个行为（上下左右跳跑等whatever），这个行为会导致环境发生变化，环境的变化会给予智能体一个反馈，让它能够计算自己距离目标（广义上的距离，不一定是位置的移动，也可能是击中数击杀数最终胜利等whatever），来判断和抉择下一步行为。</p>
<p>要设计这样一个智能体，我们理所当然地想到<strong>强化学习</strong>，至此引出本文的第一个重点概念：<strong>什么是强化学习</strong>。</p>
<p>本文将近万字，制作不易，请各位读者老爷不吝赏一个赞吧！这对我来说真的很重要，感谢感谢！</p>
<h2 id="强化学习">强化学习
</h2><h3 id="什么是强化学习">什么是强化学习
</h3><p>强化学习方法起源于动物心理学的相关原理，模仿人类和动物学习的试错机制，是通过与环境交互，学习状态到行为的映射关系，以获得最大累计期望回报的方法。状态到行为的映射关系也即策略，表示在各个状态下，智能体采取的行为或行为概率。</p>
<p>强化学习就像是人类的学习，本质上是通过与环境交互进行学习，智能体会从与环境交互的过程中获得反馈，可能是正反馈也可能是负反馈，这种反馈经验刺激智能体去拟合出一条奖励更多或者损失更小的路线。从交互中学习就是强化学习的理论基础概念。</p>
<p>强化学习是通过智能体与环境的交互，学习状态到行为之间的映射关系，因此，它包括<strong>智能体</strong>和<strong>环境</strong>两大对象。同时，还需要注意的是，强化学习是与时间序列有关的。</p>
<h3 id="智能体">智能体
</h3><p>智能体，指的是要完成某项任务的对象，强化学习的最终目的，是让智能体通过大量的学习，能够在环境中完成某个目的。在一个离散的时间序列t=0,1,2&hellip;.中，智能体会在每一个时间$t_n$内，从环境中接收一个状态$s_t$和回报$r_t$，并通过$r_t$和当前自身所处的状态$s_t$，以及已产生的经验教训判断，产生下一步$a_{t+1}$，以期望获得最大的回报。</p>
<h3 id="环境">环境
</h3><p>环境，指的是智能体所处的环境环境范围，其包含智能体可变更的状态集合以及每一种状态所获得的回报，例如：环境可以是一个二维的4*4地图，其中包含了智能体可以行进的16个位置以及每个位置对应的智能体到达该位置时可获得的回报。在离散的时间序列t=0,1,2,&hellip;中，智能体会在每个时间$t_n$内进行相应的行为$a_t$，该行为会使智能体的环境发生改变（也可能不会发生改变），环境会向智能体反馈当前的状态$s_t$和回报$r_t$。</p>
<h3 id="强化学习原理">强化学习原理
</h3><p>智能体不会被告知其在当前状态下，下一步行为应当采取何种行为，只能通过不断地尝试每一种动作，并收集环境中给予的反馈，来改善自己的行为，其改善行为的本质，就是追求回报最大化或损失最小化，在经过不断地尝试（多次迭代）后，智能体的行为会收敛成某个路径，即学习到最优的行为方式。</p>
<p>当然，这仅是笼统的强化学习原理，智能体作为一个学习行为的对象，其不一定只能通过试完每一步行为才能收敛，开发者可以通过一些算法，来加速最优行为的收敛。</p>
<h2 id="q-learning">Q-learning
</h2><p>Q-learning 是一种强化学习算法，其核心是一个叫Q表的表格Q(state, action)，这个表格一轴表示state（状态），一轴表示action（行为），<strong>Q(i,j) = k</strong>就表示智能体在 i 这个状态下，如果采取 j 这个行为会带来奖励 k ，Q表示Q-learning算法的核心，用来记录不同状态下不同行为的收益，随着迭代次数的增加，Q函数会被拟合得越来越好直至收敛或者达到指定的迭代次数。</p>
<p>下面笔者将分别介绍智能体与环境两个对象，并以gym库提供的模拟游戏环境FrozenLake为例，实现Q-learning算法，训练一个智能体。</p>
<h2 id="q-learning智能体">Q-learning智能体
</h2><p>我们知道，Q-learning算法是一种强化学习算法，强化学习有<strong>智能体</strong>与<strong>环境</strong>两个对象，本小节我们就来介绍Q-learning智能体。</p>
<p>Q-learning智能体自身主要是有两个主要动作：<strong>选择行为</strong>，<strong>更新Q表</strong>。</p>
<ul>
<li>选择行为：指的是智能体在接收外部给予的信息后，选择下一步行动的过程，其中，智能体需判断是否进行探索？如果不探索，该以何种方式来选择行为？</li>
<li>更新Q表：在选择行为后，智能体会从外部环境接收信息，然后根据外部环境提供的回报，状态等信息来更新Q表。更新Q表的函数笔者会在后面给出并详细解释。</li>
</ul>
<h3 id="参数解释">参数解释
</h3><p>Q-learning智能体自身维护了一个Q表，在初始化Q-learning时会初始化自身的Q表，与此同时，智能体还需输入几个参数：学习率、折扣因子（奖励衰减）、贪婪度。</p>
<p>在解释上面几个参数之前，我们要先给出几个词的定义，以避免在解释参数时产生误解：</p>
<ul>
<li><strong>回报</strong>：回报指的是环境给予智能体的反馈，是外部输入，比如：掉进冰湖为-1，到达终点为1，正常路径为0等，当智能体处于某一状态时，，就算它在原地打转或再次回到此处，它所获得的回报是相同的。</li>
<li><strong>奖励</strong>：奖励指的是智能体自身Q表中的记录，是内部变化，记录其在s状态，a行为下的奖励，Q表的值可能会随着智能体不断地运动，时间不断地推移而发生改变，即Q表更新，在下文中我们会给出Q表更新函数。智能体的每一次行动，都会产生一次Q表更新。Q表的值是动态的，会改变的，随着迭代次数增加，慢慢会出现，某些状态下的某些行为会更具奖励，促使智能体去更愿意尝试那样的行为，这就是“收敛”。读者也可以将<strong>奖励</strong>理解为<strong>经验</strong>。</li>
<li><strong>探索</strong>：探索指的是当智能体进行贪婪策略时，会跳出原定的行为策略，尝试随机地探索其它区域，适当的探索有利于使智能体尝试新的未知的状态，而不束缚于“经验主义”。</li>
</ul>
<p>OK，当你理解这几个词的意思后，我们现在开始解释参数：</p>
<ul>
<li><strong>学习率</strong>：学习率这个参数是各大机器学习算法的常客，在这里表示智能体对所获得的经验的学习程度，一般在属于[0,1]，学习率越大，会使得智能体跨的步伐越大，使之收敛速度越快，但注意，步伐越大可能存在的误差越大，导致无法正确收敛（陷入局部最优），损失会震荡甚至变大。反之，学习率越小，步伐越小，收敛的速度越慢。学习率是一个超参数，即由程序员事先拟定，在整个迭代过程中不可更改。</li>
<li><strong>折扣因子</strong>：折扣因子也叫奖励衰减，是对未来奖励的衰减值，当衰减值为0时，则代表智能体<strong>近视</strong>，看不见下一状态的最优行为，即智能体只在乎当前状态下环境给出的回报，而不在乎下一状态的奖励。换而言之，智能体只在乎眼下能获得的回报。当衰减值为1时，代表智能体带了眼镜，能够<strong>看得更远</strong>，它完完整整地看清楚了下一步自己会获得多大的奖励，而不仅仅只看见眼前的回报。</li>
<li><strong>贪婪度</strong>：贪婪度表示智能体尝试其它行为的可能性，如果贪婪度为0，则说明智能体只会考虑已有的奖励来选择行为，而贪婪度越高，则智能体越有可能随机选择一个行为（探索），而不是根据奖励来选择，为什么贪婪度是必要的呢，是为了避免智能体陷入局部最优解而无法收敛，随机的动作可以让智能体有机会跳出局部最优解，但务必把握好平衡，太过贪婪也可能导致智能体的行为混乱而无法收敛。</li>
</ul>
<p>下面给出智能体的初始化方法：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="c1"># 初始化，参数：状态数，动作数，学习率，折扣因子（奖励衰减），贪婪度</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_states</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_states</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">))</span>      <span class="c1">#初始化Q表，Q表的初始化为0，也可以初始化为随机值</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>                  <span class="c1">#初始化学习率，学习率默认为0.1</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">=</span> <span class="n">gamma</span>                        <span class="c1">#初始化折扣因子，折扣因子默认为0.99</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>                              <span class="c1">#初始化贪婪度，贪婪度默认为0.1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 打印初始化Q表</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;initial q_table:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>由于笔者的模拟环境为gym库提供的ForzenLake，这里的n_states是冰湖的地图大小，即4 * 4 = 16个格子，行为数为4，分别是向左，向下，向右，向上移动。</p>
<h3 id="行为选择">行为选择
</h3><p>现在我们来看一看智能体是如何选择自己的行为的，智能体的行为选择策略原理非常简单，大伙简单看看，理解就行。</p>
<p>当智能体到达某一状态（位置）时，首先会开始选择行为，即接下来我该往哪个方向走。但在动脚起身之前，我们先要抛一个硬币，诶，就是贪婪度，在上面的初始化过程中，我们将贪婪度设置为0.1，即我们获取一个随机数，当数小于0.1时，则采取探索策略，当数大于0.1时，我们按照正常的策略前进，毕竟人不能太贪婪嘛（智能体也是）。</p>
<p>最简单的贪婪策略就是随机，即随机选择一个行为，根本不在乎Q表上记录了哪一行为奖励最高（就是任性！）。当然你也可以选择其它的贪婪策略，根据自己的需要选择，但切记，贪婪的目的是为了让智能体尝试采取其它行为，不束缚于“经验主义”，陷入局部最优解之中。试想以一下，当智能体来到一个岔路口，它曾经走过向左的方向，也确实从左方能够到达终点获得相应的回报，但从上帝视角来看，往右走会有一条更近的路抵达终点，但是由于智能体没去过，且贪婪度为0，那么它将“永远”不会尝试这条路。</p>
<p>正常策略即通过Q表，来选择最优行为了，因为智能体会根据环境的反馈以及Q表更新函数，来记录哪一个行为它曾经试过且“最好”，因此，它会选择那个最优的行为，也就是上面那个例子中的“向左走”，因为它曾经确实走通过这条路，并拿到过不错的回报。</p>
<p>我们在这里并没有说贪婪策略就一定好，或者正常策略就一定好，两种策略是互补的关系，就像现实社会中，总要有人在已探明的道路上巩固地基，也要有人跃跃欲试去探索未知的道路，关键在于开发智能体的你，是否能够平衡两者。</p>
<p>下面给出行为选择的方法：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="c1"># 选择动作，参数：状态</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 判断贪婪度</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果进入了贪婪状态，那么agent将会尝试随机选择一个动作，此步称之为【探索】</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 选择最优动作，即在Q表中该状态下最大的Q值，此步称之为【利用】  </span>
</span></span><span class="line"><span class="cl">            <span class="n">state_all</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 根据Q值最大原则选取action，如果有多个action的Q相同且最大，则随机选取一个</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">state_all</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">state_all</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">max_indices</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="q表更新">Q表更新
</h3><p>终于到Q表更新了，这个更新是Q-learning算法的一大难点，也牵扯到上面的两大参数学习率与折扣因子，因此我们必须好好聊聊这个函数：
</p>
$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha[R_{t+1} + \gamma max_{a'}(s_{t+1},a') - Q(s_t,a_t)]
$$<ul>
<li>$Q(s_t,a_t)$：指的是Q表中，当前状态$s_t$，行为$a_t$时的奖励</li>
<li>$\alpha$：指的是学习率</li>
<li>$R_{t+1}$：指的是下一个状态$S_{t+1}$下，环境给予智能体的回报</li>
<li>$\gamma$：指的是折扣因子，也叫奖励衰减</li>
<li>$max_{a&rsquo;}(s_{t+1},a&rsquo;)$：指的是奖励， 实际上是一个Q表中的值，该值是下一个状态$s_{t+1}$的所有行为$a_0,a_1,a_2,&hellip;$中，最大的一个。</li>
</ul>
<p>Q表更新函数与上面的参数结合起来看，有助于你理解什么是<strong>步伐</strong>，什么是<strong>奖励</strong>，什么是<strong>回报</strong>，请允许我不厌其烦再唠叨一遍：</p>
<ul>
<li>$Q(s_t,a_t)$：是当前状态，当前行为的奖励</li>
<li>$max_{a&rsquo;}(s_{t+1},a&rsquo;)$：是下一步状态，所有行为中的最大奖励。</li>
<li>$R_{t+1}$：下一步状态的回报</li>
<li>$R_{t+1} + \gamma max_{a&rsquo;}(s_{t+1},a&rsquo;) - Q(s_t,a_t)$：步伐，即此次Q表需要更新的幅度</li>
</ul>
<p>Q表更新的难理解处：</p>
<p>笔者一开始没有搞清楚Q表的更新逻辑，这条Q表更新函数中的$max_{a&rsquo;}(s_{t+1},{a&rsquo;})$究竟怎么计算的，其实当我们知道了<strong>什么时候开始更新Q表</strong>时，这个Q表更新函数就可以看得懂了：</p>
<ul>
<li>在经过 t 时间后，智能体来到了位置（状态）$s_t$处，此时，智能体会先选择下一个行为，但请注意，此时智能体还未动身前往下一个位置</li>
<li>通过环境反馈，智能体<strong>要</strong>获取当采取某一行为后，判断出下一位置的状态$s_{t+1}$，以及可获得的回报$R_{t+1}$，但请注意，此时智能体还停留在$s_t$处，也就是智能体先不更新自己的位置，而是先从$s_{t+1}$处获取相应的信息更新Q表。</li>
<li>更新Q表</li>
<li>智能体动身前往下一位置（状态）</li>
</ul>
<p>因此，我们可知$max_{a&rsquo;}(s_{t+1},a&rsquo;)$表示的是，智能体在<strong>选择</strong>下一步行为后得到的下一步状态，那个状态下的最优行为。总而言之，智能体的奖励逻辑是，根据下一状态中4个动作中的最大奖励值，来计算出当前的这个状态所执行的这个动作的奖励值，这样的计算方式避免智能体只看当前状态的回报，也要根据“历史经验”适当考虑未来的收益。如果当前状态的回报丰厚，但是根据Q表的记录，未来下一步的奖励并不理想，也会拉低智能体对当前这一状态的“印象”。使之不会在Q表中给予很高的分数。</p>
<p>这里可能有些同学觉得过于啰嗦，但这是笔者踩过坑且困惑的地方，为了避免其它小伙伴也对此函数产生困惑，笔者还是要尽量解释一番。</p>
<p>我们来看下面这个例子，先看<code>Frozen Lake</code>的4*4地图：</p>
<p><img src="/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image-20250120164749605.png"
	width="409"
	height="445"
	srcset="/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image-20250120164749605_hu_960039d7a132a4eb.png 480w, /p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image-20250120164749605_hu_14fe7c79fe4a9156.png 1024w"
	loading="lazy"
	
		alt="Frozen Lake"
	
	
		class="gallery-image" 
		data-flex-grow="91"
		data-flex-basis="220px"
	
></p>
<p>在这个地图中，智能体出发点为左上角的格子，即第1行第1列处，此处对应Q表的第1行。终点在右下角的格子，即第4行第4列处，此处对应Q表的第16行。Q表将4*4的地图划为一个16个数据的一维数组。每个格子表示一个状态，所以我们可以说：状态即智能体所在的位置。在每个状态中，智能体可以执行4个操作，即：向左，向下，向右，向上移动，移动后，智能体会获得相应的奖励，如下所示：</p>
<p><img src="/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image-20250120164631144.png"
	width="757"
	height="504"
	srcset="/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image-20250120164631144_hu_f1bb48b45d2890a1.png 480w, /p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image-20250120164631144_hu_23309071f0f9e87c.png 1024w"
	loading="lazy"
	
		alt="Q_table"
	
	
		class="gallery-image" 
		data-flex-grow="150"
		data-flex-basis="360px"
	
></p>
<p>在上述Q表中可以看到，第16行，即最下方的[0,0,0,0]，是终点，到了此处，显然就不需要再移动了，因此此处的向左，向下，向右，向上移动的奖励值都为0。</p>
<p>我们以第15行的数据来举例说明，第15行的数据为[0, -1, 0.8, 0]，此处是第4行，第3列的位置，即<code>Forzen Lake</code>地图中终点左边的格子。因此可以看到，在这个格子中，如果智能体向右走一步，即可达到终点，因此当智能体在这个状态下尝试向右走，那么根据Q表的更新运算规则：
</p>
$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha[R_{t+1} + \gamma max_{a'}(s_{t+1},a') - Q(s_t,a_t)]
$$<p>
显然，从初始化开始时，智能体是没有在这个位置（状态）下向右（行为）行走过的，在茫茫多次选择后，某一次，智能体恰好来到这个位置（状态），它打算向右（行为）行走试试。确定要向右走后，它开始计算自己想向右走的这一个行为会获得多少奖励。</p>
<p>我们开始算一下：</p>
<p>在此时，$Q(s_t,a_t)$为0，因为自初始化伊始，智能体还没有达到过终点，而我们定下的规则，只有达到终点，才有一个奖励“1”，其它的状态都没有奖励“0”。因此还没有尝试过在此处向右走的智能体原本的该状态该动作奖励值为<code>0</code>。</p>
<p>学习率$\alpha$我们定为<code>0.8</code>。</p>
<p>智能体很惊喜地发现，下一步就是终点，因此下一步可获得的奖励$R_{t+1}$为<code>1</code>。</p>
<p>奖励衰减因子$\gamma$我们定为<code>0.99</code>。</p>
<p>因为下一个状态就是终点，此前智能体从未到达，即使到达，它也无需在此处做多余的动作，也无需再走下一状态，因此下一个状态的所有动作的奖励值都为0，因此$max_{a&rsquo;}(s_{t+1},a&rsquo;)$为<code>0</code>。</p>
<p>我们可以得到算式：$Q(s_t,a_t) = 0 + 0.8\times(1 + 0.99\times0 - 0)=0.8$</p>
<p>回头再看看Q表，是不是发现，在第15行的第3个元素的数据为0.8。没错，这个Q表就是在智能体第一次访问终点时得到的Q表。</p>
<p>你可能会感到奇怪，为什么明明智能体已经到达了终点，为什么这一趟旅程仅仅只是让Q表更新了一个有用的元素。</p>
<p>对，第一次到达终点，仅仅会让智能体的<strong>上一个状态的动作</strong>获得奖励，而非整条路线都获得奖励。但我们从宏观上来看，原本仅仅在终点才有<code>1</code>的奖励值，现在在终点旁的一个位置$S_{t-1}$，也有<code>0.8</code>的奖励值了，这多出来的具有正向奖励值的点会有什么作用呢？</p>
<p>如果我们把整个地图看作是一个海洋，终点是密度最大的地方，在一开始，除终点外，每个地方的密度都是一样的，智能体不知道该往哪个地方移动，只能随机尝试，只有当智能体非常靠近密度最大的终点时，智能体才会意识到，旁边有个疑似终点的地方。而当智能体到达终点$S_{t}$一次后，它会尝试记录下之前的动作，但肯定不是整条路线都记下来（指不定它绕了几次弯，走了几次胡同），因此它只记录最后一个步骤。怎么记录呢，就改变最后一个步骤的地区$S_{t-1}$的密度，当然不能改成和终点一样，那么下次就无法分辨哪个是终点了。因此，现在又多了一个地方的密度变大了，智能体下一次从起始点触发，就有更大的概率碰到密度不一样的地方，当重新来到$S_{t-1}$时那智能体就知道了，这个地方它留下过痕迹，那它就更愿意往这个地方走，同时嘞，它会重复上面的操作，基于这个地方，改变上一个地区$S_{t-2}$的密度，然后再检查这个地区$S_{t-1}$，当它开始检查这个地方$S_{t-1}$，嘿，您猜怎么着，它又发现了一个密度更大的地方（终点$S_{t}$）。这样，基于终点，慢慢地往外其它地方的密度都发生了变化，发生变化是基于曾经智能体来过这个区域，且它曾成功地通过该区域进入过终点的。</p>
<p>慢慢地，智能体进入终点$S_{t}$的次数越来越多，修改$S_{t-1}$的次数也越来越多，$S_{t-1}$的密度慢慢变得越来越大，也越来越接近终点$S_{t}$的密度了，同时呢，可能$S_{t-2}$与$S_{t-3}$都可以进入$S_{t-1}$，但如果$S_{t-2}$比$S_{t-3}$更容易进入$S_{t-1}$：（$P(S_{t-2}|S_{t-1})&gt;P(S_{t-3}|S_{t-1})$），那么$S_{t-2}$密度上升的速度就会比$S_{t-3}$快，对于智能体来说，这个$S_{t-2}$比$S_{t-3}$更像终点（或者更接近终点），因此形成一个正反馈，越来越多地走$S_{t-2}$ →$S_{t-1}$→$S_{t}$这条路线。</p>
<p>一个离终点两格远，但距离$S_{t-1}$仅一格远的位置$S_{t-2}$的密度，也慢慢接近$s_{t-1}$且比$s_{t-3}$的密度更大。慢慢地，随着迭代次数的增加，在海洋中，就出现了一条密度明显高于其它地方的路径，就算智能体重新回到起点，它也能根据这一条路径快速走到终点，而不必像一开始时乱走一通。这样我们就称之为收敛了，也就是训练完成。</p>
<p>小结，Q表就是智能体的小本本，它在不断地试错中，渐渐摸索出了自己的行为方式，在$s_0$时采取$a_{i0}$的方式最好，在$s_1$时采取$a_{i1}$的行为最好等等&hellip;&hellip;它小本本中的奖励，是从终点（有回报点）不断向外延伸的，而非到达终点后，此次到达终点所经历的所有点，采取的所有行为都会获得奖励。其次，大家要理解Q表的更新函数，这是Q-learning算法在重要的环节之一，更新函数所考虑的不仅仅是当前状态下的回报，也不好高骛远，只看未来不看当下，如何平衡奖励衰减与学习率，就要靠大家慢慢摸索了。</p>
<p>下面给出更新Q表的方法：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="c1"># 更新Q表，参数：状态，动作，奖励，下一个状态，是否结束</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#下面4行代码就是Q表的更新函数</span>
</span></span><span class="line"><span class="cl">        <span class="n">best_next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">td_target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">][</span><span class="n">best_next_action</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">td_error</span> <span class="o">=</span> <span class="n">td_target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">td_error</span>
</span></span><span class="line"><span class="cl"><span class="c1">#####################  手动修改Q值，加快收敛速度，这里看似“作弊”，但是不影响Q-learning的原理 ############</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># if state == next_state:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     self.q_table[state][action] = -1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># if done:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     if reward == 0:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#         self.q_table[state][action] = -1</span>
</span></span><span class="line"><span class="cl"><span class="c1">######################  当开启手动修改Q值时，会加速迭代的速度，但会影响学习曲线 #########################</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="forzenlake模拟环境">ForzenLake模拟环境
</h2><p>gym库是由OpenAI推出的一个强化学习实验环境库，FrozenLake就是其库下的一个游戏模拟环境，简单介绍一下游戏规则：</p>
<p>环境设定为游戏角色在结冰的湖面中前进，寻找宝箱，但在冰湖中存在着一些洞，一旦掉入冰洞则失败，游戏结束，找到宝箱则成功，游戏结束。</p>
<p>以一个4*4的小冰湖地图为例：</p>
<p><img src="/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image-20250120164749605.png"
	width="409"
	height="445"
	srcset="/p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image-20250120164749605_hu_960039d7a132a4eb.png 480w, /p/q-learning%E5%BC%BA%E5%8C%96%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/image-20250120164749605_hu_14fe7c79fe4a9156.png 1024w"
	loading="lazy"
	
		alt="Frozen Lake"
	
	
		class="gallery-image" 
		data-flex-grow="91"
		data-flex-basis="220px"
	
></p>
<p>显然，在这个冰湖中，角色初始位置在左上角的位置，从二维数组上的角度来看就是[0,0]，从一维数组的角度来看就是[0]，宝藏在地图右下角，从二维数组的角度上来看就是[3,3]，从一位数组的角度上来看就是[15]，注意：上面的地图仅仅是我们人类观察者所见到的地图，而智能体看到的并不是这样的地图，它仅能够知道自己走到了第几个格子或者第几行第几列的格子，并从中获得什么回报，因此笔者才强调从数组的角度出发，因为这是智能体的角度。</p>
<p>其次，作为上帝视角的我们，可以再冰湖上增加一些特殊的机制，比如湖面可能打滑，有概率往随机的方向上行动而不遵循智能体的意志，但仅仅是示例，咱们就不做那些复杂的操作了。</p>
<p>现在我们给出了环境的地图，接下来给出环境的回报规则：</p>
<p>只有在角色找到宝藏时，才会获得回报1，掉进冰洞与冰面行走回报都为0，掉进冰洞直接结束游戏，开启下一轮迭代。</p>
<p>by the way，因为掉进冰洞和在冰面行走的回报都为0，因此智能体是分不清掉进冰洞与冰面行走的区别，因此如果你想加速迭代，让智能体明白掉进冰洞是危险的，可以将掉进冰洞的回报设置为-1，让智能体意识到这里有惩罚，下一次迭代的策略就会避开这个位置。（这也是上面更新Q表代码中我们做的一点小小的“作弊”）。</p>
<p>接下来我们给出环境的代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>	<span class="c1">#重置环境，即更新角色状态（位置）</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#print(state)</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>	
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>											 <span class="c1">#判断游戏是否结束</span>
</span></span><span class="line"><span class="cl">        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>					   <span class="c1">#智能体选择接下来的行为</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#print(env.step(action))</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>		<span class="c1">#将行为输入给环境，环境会根据行为反馈下一步状态，回报，是否结束游戏等信息</span>
</span></span><span class="line"><span class="cl">        <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>    <span class="c1">#根据环境反馈的信息以及自身的位置，智能体更新Q表  </span>
</span></span><span class="line"><span class="cl">        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>									  <span class="c1">#更新智能体的状态</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>								  <span class="c1">#将当前回报加总，在整个一轮游戏结束后，看看此轮游戏获得的总回报，判断此轮游戏的收敛效果。</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">total_reward</span>										<span class="c1"># 游戏结束后，返回本轮游戏的总回报值，可用于后续的曲线绘制和分析研究</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 使用示例</span>
</span></span><span class="line"><span class="cl"><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v1&#39;</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="s2">&#34;4x4&#34;</span><span class="p">,</span> <span class="n">is_slippery</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">render_mode</span> <span class="o">=</span> <span class="s2">&#34;human&#34;</span><span class="p">)</span>	<span class="c1">#创建冰湖环境，呈现画面</span>
</span></span><span class="line"><span class="cl"><span class="c1"># env = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&#34;4x4&#34;, is_slippery=False, render_mode = &#34;ansi&#34;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># env = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&#34;4x4&#34;, is_slippery=False)</span>
</span></span><span class="line"><span class="cl"><span class="n">agent</span> <span class="o">=</span> <span class="n">QLearningAgent</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>		<span class="c1">#初始化智能体</span>
</span></span><span class="line"><span class="cl"><span class="n">n_episodes</span> <span class="o">=</span> <span class="mi">10000</span>	<span class="c1">#迭代次数</span>
</span></span><span class="line"><span class="cl"><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>	<span class="c1">#用来记录回报曲线，展示收敛效果</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">reward</span> <span class="o">=</span> <span class="n">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>	<span class="c1">#每一次run_episode，都是一次游戏进程，只有当游戏结束（掉进冰湖或拿到宝箱），才会返回</span>
</span></span><span class="line"><span class="cl">    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>			   <span class="c1">#将本轮游戏的总体回报记录下来，以便后续绘制曲线，展示收敛效果</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在上面的代码中，我们完成了整个模拟环境的搭建以及迭代过程，基本上在每一行代码中都添加了注释，下面就不过多赘述了。reward列表和total_reward是非必要的选择，但会给我们后续研究智能体的迭代速度提供直观的数据支持，如果读者们要调参，修改策略等进阶玩法，则这一步是很重要的，当然读者们不一定只看回报这一个指标，你也可以将其它指标也收集起来，比如每一轮游戏的Q表等等，通过以下代码可以将每轮游戏的最终Q表打印出来：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 打印最终的Q表</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;final q_table:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">q_table</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>最后，我们来解释一下<code>gym.make</code>方法的各种参数：</p>
<ul>
<li>FrozenLakeLake：说明我们需要创建的模拟环境为冰湖</li>
<li>desc：作用是自定义网格地图布局，我们设置为None，即使用默认的预定义地图，如果需要自定义地图，可以传入一个字符串列表，如：[&ldquo;SFFF&rdquo;, &ldquo;FHFH&rdquo;, &ldquo;FFFH&rdquo;, &ldquo;HFFG&rdquo;]，其中：S表示起点，F表示安全冰面，H表示冰洞，G表示目标</li>
<li>map_name：选择预定的网格地图，当desc为None时生效，表明采用预定的4*4大小的地图</li>
<li>is_slippery：控制移动的随机性，True（默认），智能体执行动作时有1/3概率打滑，False则执行动作是确定的（智能体严格按照指定方向移动）</li>
<li>render_mode：设置环境渲染模式，human在屏幕上实时显示图形化界面，ansi在终端输出文本网格，None不渲染（默认）</li>
</ul>
<h2 id="总结">总结
</h2><p>上面，咱们完整地讲解了Q-learning算法的基本原理，智能体与环境如何实现，有什么难点和需要掌握的地方，总的来说，Q-learning的本质核心其实也就是强化学习的本质核心——试错！只有在不断地跌倒和捶打中，才能诞生强大的智能体！抓住这一核心，其余的部分都可以做优化和修改，包括最关键的Q表更新函数，只要能够让智能体进化以达到自身目的，将其改的面目全非甚至不是Q-learning了又何尝不可呢，笔者只是通过Q-learning来初步学习强化学习，而强化学习并非只有Q-learning。笔者也把当初自己在学习过程中遇到的难点给详细阐述了一遍，当然笔者的语言功底有限，或许有部分还未讲清楚，也欢迎各位大佬留言。当笔者进一步学习其它强化学习知识点和算法的时候，再出文章，那么，下篇文章再见啦，祝各位变得更强！</p>
<h2 id="完整代码">完整代码
</h2><p>这代码笔者参考了下面参考文章的代码，浅浅修改了一些地方，并添加了一些注释，仅供参考：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span><span class="lnt">89
</span><span class="lnt">90
</span><span class="lnt">91
</span><span class="lnt">92
</span><span class="lnt">93
</span><span class="lnt">94
</span><span class="lnt">95
</span><span class="lnt">96
</span><span class="lnt">97
</span><span class="lnt">98
</span><span class="lnt">99
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">gym</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QLearningAgent</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 初始化，参数：状态数，动作数，学习率，折扣因子（奖励衰减），贪婪度</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_states</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_states</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">))</span>      <span class="c1">#初始化Q表，Q表的初始化为0，也可以初始化为随机值</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>                  <span class="c1">#初始化学习率，学习率默认为0.1</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">=</span> <span class="n">gamma</span>                        <span class="c1">#初始化折扣因子，折扣因子默认为0.99</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>                              <span class="c1">#初始化贪婪度，贪婪度默认为0.1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 打印初始化Q表</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;initial q_table:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 选择动作，参数：状态</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 判断贪婪度</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果进入了贪婪状态，那么agent将会尝试随机选择一个动作，此步称之为【探索】</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 选择最优动作，即在Q表中该状态下最大的Q值，此步称之为【利用】</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># return np.argmax(self.q_table[state])</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 根据Q值最大原则选取action，如果有多个action的Q相同且最大，则随机选取一个</span>
</span></span><span class="line"><span class="cl">            <span class="n">state_all</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">state_all</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">state_all</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">max_indices</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 更新Q表，参数：状态，动作，奖励，下一个状态，是否结束</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">best_next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">td_target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">][</span><span class="n">best_next_action</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">td_error</span> <span class="o">=</span> <span class="n">td_target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">td_error</span>
</span></span><span class="line"><span class="cl"><span class="c1">#####################  手动修改Q值，加快收敛速度，这里看似“作弊”，但是不影响Q-learning的原理 ############</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># if state == next_state:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     self.q_table[state][action] = -1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># if done:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     if reward == 0:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#         self.q_table[state][action] = -1</span>
</span></span><span class="line"><span class="cl"><span class="c1">######################  当开启手动修改Q值时，会加速迭代的速度，但会影响学习曲线 #########################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>                   <span class="c1">#重置环境，即更新角色状态（位置）</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#print(state)</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>                                             <span class="c1">#判断游戏是否结束</span>
</span></span><span class="line"><span class="cl">        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>                     <span class="c1">#智能体选择接下来的行为</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#print(env.step(action))</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>       <span class="c1">#将行为输入给环境，环境会根据行为反馈下一步状态，回报，是否结束游戏等信息</span>
</span></span><span class="line"><span class="cl">        <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>   <span class="c1">#根据环境反馈的信息以及自身的位置，智能体更新Q表  </span>
</span></span><span class="line"><span class="cl">        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>                                      <span class="c1">#更新智能体的状态</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>                                  <span class="c1">#将当前回报加总，在整个一轮游戏结束后，看看此轮游戏获得的总回报，判断此轮游戏的收敛效果。</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">total_reward</span>                                         <span class="c1"># 游戏结束后，返回本轮游戏的总回报值，可用于后续的曲线绘制和分析研究</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 使用示例</span>
</span></span><span class="line"><span class="cl"><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v1&#39;</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="s2">&#34;4x4&#34;</span><span class="p">,</span> <span class="n">is_slippery</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">render_mode</span> <span class="o">=</span> <span class="s2">&#34;human&#34;</span><span class="p">)</span>    <span class="c1">#创建冰湖环境，呈现画面</span>
</span></span><span class="line"><span class="cl"><span class="c1"># env = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&#34;4x4&#34;, is_slippery=False, render_mode = &#34;ansi&#34;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># env = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&#34;4x4&#34;, is_slippery=False)</span>
</span></span><span class="line"><span class="cl"><span class="n">agent</span> <span class="o">=</span> <span class="n">QLearningAgent</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>     <span class="c1">#初始化智能体</span>
</span></span><span class="line"><span class="cl"><span class="n">n_episodes</span> <span class="o">=</span> <span class="mi">10000</span>           <span class="c1">#迭代次数</span>
</span></span><span class="line"><span class="cl"><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1">#用来记录回报曲线，展示收敛效果</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>       <span class="c1">#每一次run_episode，都是一次游戏进程，只有当游戏结束（掉进冰湖或拿到宝箱），才会返回</span>
</span></span><span class="line"><span class="cl">    <span class="n">reward</span> <span class="o">=</span> <span class="n">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>    <span class="c1">#将本轮游戏的总体回报记录下来，以便后续绘制曲线，展示收敛效果</span>
</span></span><span class="line"><span class="cl">    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="c1"># 打印最终的Q表</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;final q_table:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">q_table</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 绘制学习曲线</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Episode&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Average Reward&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Q-Learning on FrozenLake&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 测试学习到的策略</span>
</span></span><span class="line"><span class="cl"><span class="n">n_test_episodes</span> <span class="o">=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl"><span class="n">test_rewards</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_test_episodes</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Average test reward: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_rewards</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="参考文章">参考文章
</h2><p><a class="link" href="https://blog.csdn.net/universsky2015/article/details/132138174?ops_request_misc=%7B%22request%5Fid%22%3A%220aaa40bf6ecabd8c95b05e8445c140b6%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fblog.%22%7D&amp;request_id=0aaa40bf6ecabd8c95b05e8445c140b6&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-7-132138174-null-null.nonecase&amp;utm_term=%e9%81%97%e4%bc%a0%e7%ae%97%e6%b3%95&amp;spm=1018.2226.3001.4450"  target="_blank" rel="noopener"
    >智能体入门——遗传算法与Qlearning_genetic algorithm和qlearning的区别和关系-CSDN博客</a></p>
<p><a class="link" href="https://blog.csdn.net/shoppingend/article/details/124291112?ops_request_misc=%7B%22request%5Fid%22%3A%221dd02de3faea5040794f77b39343d4c6%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=1dd02de3faea5040794f77b39343d4c6&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-124291112-null-null.142%5ev101%5epc_search_result_base5&amp;utm_term=Qlearning&amp;spm=1018.2226.3001.4187"  target="_blank" rel="noopener"
    >【强化学习】Q-Learning算法详解-CSDN博客</a></p>
<p><a class="link" href="https://blog.csdn.net/qq_43774332/article/details/131083767?ops_request_misc=%7B%22request%5Fid%22%3A%221dd02de3faea5040794f77b39343d4c6%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=1dd02de3faea5040794f77b39343d4c6&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-131083767-null-null.142%5ev101%5epc_search_result_base5&amp;utm_term=Qlearning&amp;spm=1018.2226.3001.4187"  target="_blank" rel="noopener"
    >【机器学习】Q-Learning详细介绍-CSDN博客</a></p>
<p><a class="link" href="https://zhuanlan.zhihu.com/p/343668723"  target="_blank" rel="noopener"
    >强化学习之迷宫Q-Learning实践笔记——入门篇 - 知乎</a></p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/">数学建模</a>
        
            <a href="/tags/%E6%95%B0%E5%AD%A6/">数学</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">相关文章</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="has-image">
    <a href="/p/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/">
        
        
            <div class="article-image">
                <img src="/p/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/image.3494bbe150c6afbb7f1a522a98c42c87_hu_16b2543e9e878a3c.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post 遗传算法"
                        
                        data-hash="md5-NJS74VDGr7t/GlIqmMQshw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">遗传算法</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <script src="https://utteranc.es/client.js" 
        repo=""
        issue-term="pathname"
        
        crossorigin="anonymous"
        async
        >
</script>

<style>
    .utterances {
        max-width: unset;
    }
</style>

<script>
    let utterancesLoaded = false;

    function setUtterancesTheme(theme) {
        let utterances = document.querySelector('.utterances iframe');
        if (utterances) {
            utterances.contentWindow.postMessage(
                {
                    type: 'set-theme',
                    theme: `github-${theme}`
                },
                'https://utteranc.es'
            );
        }
    }

    addEventListener('message', event => {
        if (event.origin !== 'https://utteranc.es') return;

        
        utterancesLoaded = true;
        setUtterancesTheme(document.documentElement.dataset.scheme)
    });

    window.addEventListener('onColorSchemeChange', (e) => {
        if (!utterancesLoaded) return;
        setUtterancesTheme(e.detail)
    })
</script>


    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2024 - 
        
        2025 Lyrical Wander
    </section>
    
    <section class="powerby">
        
            written by LyricalWander <br/>
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.29.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
